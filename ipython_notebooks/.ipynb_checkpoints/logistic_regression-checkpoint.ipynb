{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/pandas/core/computation/__init__.py:18: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Notebook for smc sampler \n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import norm\n",
    "from scipy.special import gamma\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/home/alex/Dropbox/smc_hmc/python_smchmc/\")\n",
    "\n",
    "from smc_sampler_functions.functions_smc_help import sequence_distributions\n",
    "\n",
    "\n",
    "# define the parameters\n",
    "dim_list = [2, 5, 10, 20, 50, 100, 200, 300]\n",
    "try:\n",
    "    dim = dim_list[int(sys.argv[1])-1]\n",
    "except:\n",
    "    dim = 5\n",
    "N_particles = 2**4\n",
    "T_time = 2000\n",
    "move_steps_hmc = 1\n",
    "move_steps_rw_mala = 50\n",
    "ESStarget = 0.95\n",
    "M_num_repetions = 1\n",
    "epsilon = .005\n",
    "epsilon_hmc = .1\n",
    "#rs = np.random.seed(1)\n",
    "targetmean = np.ones(dim)*2\n",
    "targetvariance = np.eye(dim)*0.1\n",
    "targetvariance_inv = np.linalg.inv(targetvariance)\n",
    "l_targetvariance_inv = np.linalg.cholesky(targetvariance_inv)\n",
    "parameters = {'dim' : dim, \n",
    "              'N_particles' : N_particles, \n",
    "              'targetmean': targetmean, \n",
    "              'targetvariance':targetvariance,\n",
    "              'targetvariance_inv':targetvariance_inv,\n",
    "              'l_targetvariance_inv':l_targetvariance_inv,\n",
    "              'df' : 5,\n",
    "              'T_time' : T_time,\n",
    "              'autotempering' : True,\n",
    "              'ESStarget': ESStarget,\n",
    "              'adaptive_covariance' : True\n",
    "             }\n",
    "\n",
    "\n",
    "\n",
    "# define the target distributions\n",
    "#from smc_sampler_functions.cython.cython_target_distributions import priorlogdens, priorgradlogdens\n",
    "from smc_sampler_functions.target_distributions import priorlogdens, priorgradlogdens\n",
    "from smc_sampler_functions.target_distributions import targetlogdens_logistic, targetgradlogdens_logistic, f_dict_logistic_regression\n",
    "from smc_sampler_functions.target_distributions import targetlogdens_student, targetgradlogdens_student\n",
    "parameters_logistic = f_dict_logistic_regression(dim)\n",
    "from smc_sampler_functions.target_distributions import targetlogdens_logistic_help_safe\n",
    "parameters_logistic = f_dict_logistic_regression(dim)\n",
    "#import ipdb; ipdb.set_trace()\n",
    "parameters.update(parameters_logistic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-69.31471806]\n",
      "Warning: Maximum number of function evaluations has been exceeded.\n",
      "[ 0.89144232  0.6106106   0.68289645  0.5591443   0.97590696]\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "from functools import partial\n",
    "partial_target_max = partial(targetlogdens_logistic, parameters=parameters_logistic) \n",
    "def partial_target(x):\n",
    "    return(partial_target_max(x)*-1)\n",
    "x0 = np.ones((1,dim))*0\n",
    "print(partial_target_max(x0))\n",
    "targetlogdens_logistic(x0, parameters_logistic)\n",
    "res = minimize(partial_target, x0, method='nelder-mead', options={'xtol': 1e-8, 'disp': True})\n",
    "print(res.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5801041   0.41368747  0.44417985  0.38967335  0.65197806]]\n",
      "[-49.90012959]\n",
      "[-49.90012959]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 49.900130\n",
      "         Iterations: 195\n",
      "         Function evaluations: 311\n",
      "[ 0.5801041   0.41368747  0.44417984  0.38967335  0.65197805]\n",
      "[[ -3.20659332e-09   1.19006538e-08   2.97993305e-07   3.26799388e-09\n",
      "    2.08340128e-07]]\n",
      "[[ 0.5801041   0.41368747  0.44417985  0.38967335  0.65197806]] [[ 0.89127826  0.61045037  0.68281327  0.55897488  0.97564092]]\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((1,dim))\n",
    "N_particles = 1000\n",
    "particles = np.random.normal(size=(N_particles, dim))\n",
    "from scipy.stats import norm\n",
    "def targetgradlogdens_probit(particles, parameters):\n",
    "    \"\"\"\n",
    "    the gradient of the logdensity of a probit model\n",
    "    \"\"\"\n",
    "    particles = np.atleast_2d(particles)\n",
    "    y = parameters['y_all']\n",
    "    X = parameters['X_all']\n",
    "    factor_yx = (y*X)[:,:,np.newaxis]\n",
    "    factordensity = norm.pdf(X.dot(particles.transpose()))[:,np.newaxis,:]\n",
    "    factorProb = np.clip(norm.cdf(X.dot(particles.transpose()))[:,np.newaxis,:], 4e-16, 1-4e-16)\n",
    "    numerator =  factor_yx*factordensity - X[:,:,np.newaxis]*factordensity*factorProb\n",
    "    denominator = (1-factorProb)*factorProb\n",
    "    gradient_pi_0 = -particles\n",
    "    return (numerator/denominator).sum(axis=0).transpose()+gradient_pi_0\n",
    "\n",
    "\n",
    "def targetlogdens_probit(particles, parameters):\n",
    "    \"\"\"\n",
    "    the gradient of the logdensity of a probit model\n",
    "    \"\"\"\n",
    "    particles = np.atleast_2d(particles)\n",
    "    y = parameters['y_all']\n",
    "    X = parameters['X_all']\n",
    "\n",
    "    factorProb = norm.cdf(X.dot(particles.transpose()))\n",
    "    part1 = y*np.log(np.clip(factorProb, 4e-16, 1-4e-16))\n",
    "    part2 = (1-y)*np.log(1-np.clip(factorProb, 4e-16, 1-4e-16))\n",
    "    res = (part1+part2).sum(axis=0)-0.5*np.linalg.norm(particles, axis=1)**2\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "partial_target_max = partial(targetlogdens_probit, parameters=parameters_logistic) \n",
    "def partial_target(x):\n",
    "    return(partial_target_max(x)*-1)\n",
    "particles = np.ones((1,dim))*0\n",
    "print(x0)\n",
    "print(partial_target_max(x0))\n",
    "print(targetlogdens_probit(x0, parameters_logistic))\n",
    "res = minimize(partial_target, x0, method='nelder-mead', options={'xtol': 1e-8, 'disp': True})\n",
    "print(res.x)\n",
    "print(targetgradlogdens_probit(res.x, parameters))\n",
    "x0prob = np.ones((1,dim))*0\n",
    "x0log = np.ones((1,dim))*0\n",
    "for t in range(100):\n",
    "    x0prob = x0prob +0.01*targetgradlogdens_probit(x0prob, parameters)\n",
    "    x0log = x0log +0.01*targetgradlogdens_logistic(x0log, parameters)\n",
    "print(x0prob, x0log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-35.45506712678484"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles = np.random.normal(size=(N_particles, dim))\n",
    "print(np.isfinite(targetgradlogdens_probit(particles, parameters)).sum())\n",
    "print(np.isfinite(targetlogdens_probit(particles, parameters)).sum())\n",
    "factorProb = norm.cdf(parameters['X_all'].dot(particles.transpose()))\n",
    "np.min(np.log(1-factorProb+4e-16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22.0321887   14.46110005  24.16042122   9.73651666  21.76487897]]\n",
      "[[ 22.03218874  14.46109951  24.16042125   9.73651502  21.76487952]]\n",
      "[[ 21.05657894  13.48549029  23.18481146   8.7609069   20.78926921]]\n"
     ]
    }
   ],
   "source": [
    "def approx_gradient(function, x, h=0.00000001):\n",
    "    dim = x.shape[1]\n",
    "    grad_vector = np.zeros(x.shape)\n",
    "    for i in range(dim):\n",
    "        x_1 = np.copy(x)\n",
    "        x_2 = np.copy(x)\n",
    "        x_1[:,i] = x[:,i]+h\n",
    "        x_2[:,i] = x[:,i]-h\n",
    "        grad_vector[:,i] = (function(x_1)-function(x_2))/(2*h)\n",
    "    return(grad_vector)\n",
    "print(approx_gradient(partial_target_max, x0))\n",
    "print(targetgradlogdens_probit(x0, parameters))\n",
    "print(approx_gradient(partial_target_max, x0) - targetgradlogdens_student(x0, parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.89143054,  0.61061055,  0.68289088,  0.55914027,  0.97590728]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles = np.zeros(parameters['dim'])\n",
    "targetlogdens_logistic(particles, parameters_logistic)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(fit_intercept =  False)\n",
    "log_reg.fit(parameters_logistic['X_all'], parameters_logistic['y_all'])\n",
    "log_reg.get_params()\n",
    "log_reg.coef_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
