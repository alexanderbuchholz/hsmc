{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notebook for smc sampler \n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import norm\n",
    "from scipy.special import gamma\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from smc_sampler_functions.functions_smc_help import sequence_distributions\n",
    "\n",
    "\n",
    "# define the parameters\n",
    "dim_list = [2, 5, 10, 20, 50, 100, 200, 300]\n",
    "try:\n",
    "    dim = dim_list[int(sys.argv[1])-1]\n",
    "except:\n",
    "    dim = 5\n",
    "N_particles = 2**4\n",
    "T_time = 2000\n",
    "move_steps_hmc = 1\n",
    "move_steps_rw_mala = 50\n",
    "ESStarget = 0.95\n",
    "M_num_repetions = 1\n",
    "epsilon = .005\n",
    "epsilon_hmc = .1\n",
    "#rs = np.random.seed(1)\n",
    "targetmean = np.ones(dim)*2\n",
    "targetvariance = np.eye(dim)*0.1\n",
    "targetvariance_inv = np.linalg.inv(targetvariance)\n",
    "l_targetvariance_inv = np.linalg.cholesky(targetvariance_inv)\n",
    "parameters = {'dim' : dim, \n",
    "              'N_particles' : N_particles, \n",
    "              'targetmean': targetmean, \n",
    "              'targetvariance':targetvariance,\n",
    "              'targetvariance_inv':targetvariance_inv,\n",
    "              'l_targetvariance_inv':l_targetvariance_inv,\n",
    "              'df' : 5,\n",
    "              'T_time' : T_time,\n",
    "              'autotempering' : True,\n",
    "              'ESStarget': ESStarget,\n",
    "              'adaptive_covariance' : True\n",
    "             }\n",
    "\n",
    "\n",
    "\n",
    "# define the target distributions\n",
    "#from smc_sampler_functions.cython.cython_target_distributions import priorlogdens, priorgradlogdens\n",
    "from smc_sampler_functions.target_distributions import priorlogdens, priorgradlogdens\n",
    "from smc_sampler_functions.target_distributions import targetlogdens_logistic, targetgradlogdens_logistic, f_dict_logistic_regression\n",
    "from smc_sampler_functions.target_distributions import targetlogdens_student, targetgradlogdens_student\n",
    "parameters_logistic = f_dict_logistic_regression(dim)\n",
    "from smc_sampler_functions.target_distributions import targetlogdens_logistic_help_safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def targetlogdens_logistic_help(particles, X, y):\n",
    "    \"\"\"\n",
    "    likelihood of the logistic regression\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(X, particles.transpose())\n",
    "    #sigmoid_value = logplus_one(dot_product)\n",
    "    sigmoid_value = np.log(1+np.exp(-dot_product))\n",
    "    likelihood_value = (-y*sigmoid_value + (1-y)*(dot_product+sigmoid_value)).sum(axis=0)\n",
    "    return likelihood_value-np.linalg.norm(particles)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-69.31471806]\n",
      "Warning: Maximum number of function evaluations has been exceeded.\n",
      "[-232.89232507  161.99845377 -143.44916357   76.12906628  -61.46971154]\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "from functools import partial\n",
    "partial_target_max = partial(targetlogdens_logistic, parameters=parameters_logistic) \n",
    "def partial_target(x):\n",
    "    return(partial_target_max(x)*-1)\n",
    "x0 = np.ones((1,dim))*0\n",
    "print(partial_target_max(x0))\n",
    "targetlogdens_logistic_help_safe(x0, parameters_logistic['X_all'], parameters_logistic['y_all'])\n",
    "res = minimize(partial_target_max, x0, method='nelder-mead', options={'xtol': 1e-8, 'disp': True})\n",
    "print(res.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.6368805  -6.21344682 -4.29220961 -6.19609466  4.19542765]]\n",
      "[[ 1.81818182  1.81818182  1.81818182  1.81818182  1.81818182]]\n",
      "[[-0.18130132 -8.03162864 -6.11039143 -8.01427648  2.37724583]]\n"
     ]
    }
   ],
   "source": [
    "def approx_gradient(function, x, h=0.00000001):\n",
    "    dim = x.shape[1]\n",
    "    grad_vector = np.zeros(x.shape)\n",
    "    for i in range(dim):\n",
    "        x_1 = np.copy(x)\n",
    "        x_2 = np.copy(x)\n",
    "        x_1[:,i] = x[:,i]+h\n",
    "        x_2[:,i] = x[:,i]-h\n",
    "        grad_vector[:,i] = (function(x_1)-function(x_2))/(2*h)\n",
    "    return(grad_vector)\n",
    "print(approx_gradient(partial_target_max, x0))\n",
    "print(targetgradlogdens_student(x0, parameters))\n",
    "print(approx_gradient(partial_target_max, x0) - targetgradlogdens_student(x0, parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.89143054,  0.61061055,  0.68289088,  0.55914027,  0.97590728]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles = np.zeros(parameters['dim'])\n",
    "targetlogdens_logistic(particles, parameters_logistic)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(fit_intercept =  False)\n",
    "log_reg.fit(parameters_logistic['X_all'], parameters_logistic['y_all'])\n",
    "log_reg.get_params()\n",
    "log_reg.coef_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
